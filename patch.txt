diff --git a/cluster/src/main/scala/io/snappydata/impl/LeadImpl.scala b/cluster/src/main/scala/io/snappydata/impl/LeadImpl.scala
index ff292f9ff..47df14350 100644
--- a/cluster/src/main/scala/io/snappydata/impl/LeadImpl.scala
+++ b/cluster/src/main/scala/io/snappydata/impl/LeadImpl.scala
@@ -187,8 +187,7 @@ class LeadImpl extends ServerImpl with Lead
 
       conf.setAll(bootProperties.asScala)
       // set spark ui port to 5050 that is snappy's default
-      conf.set("spark.ui.port",
-        bootProperties.getProperty("spark.ui.port", LeadImpl.SPARKUI_PORT.toString))
+      conf.setIfMissing("spark.ui.port", LeadImpl.SPARKUI_PORT.toString)
 
       // wait for log service to initialize so that Spark also uses the same
       while (!ClientSharedUtils.isLoggerInitialized && status() != State.RUNNING) {
diff --git a/core/src/main/scala/io/snappydata/util/ServiceUtils.scala b/core/src/main/scala/io/snappydata/util/ServiceUtils.scala
index df83d8893..18c09ace8 100644
--- a/core/src/main/scala/io/snappydata/util/ServiceUtils.scala
+++ b/core/src/main/scala/io/snappydata/util/ServiceUtils.scala
@@ -16,6 +16,7 @@
  */
 package io.snappydata.util
 
+import java.nio.file.{Files, Paths}
 import java.util.Properties
 import java.util.regex.Pattern
 
@@ -30,9 +31,10 @@ import _root_.com.pivotal.gemfirexd.internal.engine.distributed.utils.GemFireXDU
 import io.snappydata.{Constant, Property, ServerManager, SnappyTableStatsProviderService}
 
 import org.apache.spark.memory.MemoryMode
+import org.apache.spark.sql.collection.Utils
+import org.apache.spark.sql.hive.HiveClientUtil
 import org.apache.spark.sql.{SnappyContext, SparkSession, ThinClientConnectorMode}
 import org.apache.spark.{SparkContext, SparkEnv}
-import org.apache.spark.sql.collection.Utils
 
 /**
  * Common utility methods for store services.
@@ -42,7 +44,7 @@ object ServiceUtils {
   val LOCATOR_URL_PATTERN: Pattern = Pattern.compile("(.+:[0-9]+)|(.+\\[[0-9]+\\])")
 
   private[snappydata] def getStoreProperties(
-      confProps: Seq[(String, String)]): Properties = {
+      confProps: Seq[(String, String)], forInit: Boolean = false): Properties = {
     val storeProps = new Properties()
     confProps.foreach {
       case (Property.Locators(), v) =>
@@ -62,44 +64,44 @@ object ServiceUtils {
           k.startsWith(Constant.JOBSERVER_PROPERTY_PREFIX) => storeProps.setProperty(k, v)
       case _ => // ignore rest
     }
-    setCommonBootDefaults(storeProps, forLocator = false)
+    setCommonBootDefaults(storeProps, forLocator = false, forInit)
   }
 
   private[snappydata] def setCommonBootDefaults(props: Properties,
-      forLocator: Boolean): Properties = {
+      forLocator: Boolean, forInit: Boolean = true): Properties = {
     val storeProps = if (props ne null) props else new Properties()
     if (!forLocator) {
       // set default recovery delay to 2 minutes (SNAP-1541)
-      if (storeProps.getProperty(GfxdConstants.DEFAULT_STARTUP_RECOVERY_DELAY_PROP) == null) {
-        storeProps.setProperty(GfxdConstants.DEFAULT_STARTUP_RECOVERY_DELAY_PROP, "120000")
-      }
+      storeProps.putIfAbsent(GfxdConstants.DEFAULT_STARTUP_RECOVERY_DELAY_PROP, "120000")
       // try hard to maintain executor and node locality
-      if (storeProps.getProperty("spark.locality.wait.process") == null) {
-        storeProps.setProperty("spark.locality.wait.process", "20s")
-      }
-      if (storeProps.getProperty("spark.locality.wait") == null) {
-        storeProps.setProperty("spark.locality.wait", "10s")
-      }
+      storeProps.putIfAbsent("spark.locality.wait.process", "20s")
+      storeProps.putIfAbsent("spark.locality.wait", "10s")
       // default value for spark.sql.files.maxPartitionBytes in snappy is 32mb
-      if (storeProps.getProperty("spark.sql.files.maxPartitionBytes") == null) {
-        storeProps.setProperty("spark.sql.files.maxPartitionBytes", "33554432")
-      }
+      storeProps.putIfAbsent("spark.sql.files.maxPartitionBytes", "33554432")
 
+      // change hive temporary files location to be inside working directory
+      // to fix issues with concurrent queries trying to access/write same directories
+      if (forInit) {
+        Utils.deletePath(Paths.get(HiveClientUtil.HIVE_TMPDIR))
+        Files.createDirectories(Paths.get(HiveClientUtil.HIVE_TMPDIR))
+      }
+      // set as system properties so that these can be overridden by
+      // hive-site.xml if required
+      val sysProps = System.getProperties
+      for ((hiveVar, dirName) <- HiveClientUtil.HIVE_DEFAULT_SETTINGS) {
+        sysProps.putIfAbsent(hiveVar.varname, dirName)
+      }
     }
     // set default member-timeout higher for GC pauses (SNAP-1777)
-    if (storeProps.getProperty(DistributionConfig.MEMBER_TIMEOUT_NAME) == null) {
-      storeProps.setProperty(DistributionConfig.MEMBER_TIMEOUT_NAME, "30000")
-    }
+    storeProps.putIfAbsent(DistributionConfig.MEMBER_TIMEOUT_NAME, "30000")
     // set network partition detection by default
-    if (storeProps.getProperty(ENABLE_NETWORK_PARTITION_DETECTION_NAME) == null) {
-      storeProps.setProperty(ENABLE_NETWORK_PARTITION_DETECTION_NAME, "true")
-    }
+    storeProps.putIfAbsent(ENABLE_NETWORK_PARTITION_DETECTION_NAME, "true")
     storeProps
   }
 
   def invokeStartFabricServer(sc: SparkContext,
       hostData: Boolean): Unit = {
-    val properties = getStoreProperties(Utils.getInternalSparkConf(sc).getAll)
+    val properties = getStoreProperties(Utils.getInternalSparkConf(sc).getAll, forInit = true)
     // overriding the host-data property based on the provided flag
     if (!hostData) {
       properties.setProperty("host-data", "false")
diff --git a/core/src/main/scala/org/apache/spark/sql/collection/Utils.scala b/core/src/main/scala/org/apache/spark/sql/collection/Utils.scala
index f77c6ce05..b7cea6da1 100644
--- a/core/src/main/scala/org/apache/spark/sql/collection/Utils.scala
+++ b/core/src/main/scala/org/apache/spark/sql/collection/Utils.scala
@@ -19,6 +19,7 @@ package org.apache.spark.sql.collection
 import java.io.ObjectOutputStream
 import java.lang.reflect.Method
 import java.nio.ByteBuffer
+import java.nio.file.{Files, Path}
 import java.sql.{DriverManager, ResultSet}
 import java.util.TimeZone
 
@@ -68,7 +69,7 @@ import org.apache.spark.util.AccumulatorV2
 import org.apache.spark.util.collection.BitSet
 import org.apache.spark.util.io.ChunkedByteBuffer
 
-object Utils {
+object Utils extends Logging {
 
   final val EMPTY_STRING_ARRAY = SharedUtils.EMPTY_STRING_ARRAY
   final val WEIGHTAGE_COLUMN_NAME = "snappy_sampler_weightage"
@@ -870,6 +871,36 @@ object Utils {
       case _ => false
     }
   }
+
+  def deletePath(path: Path, throwOnError: Boolean = false,
+      logIfNotExists: Boolean = false): Unit = {
+    if (Files.exists(path)) {
+      var failure: Exception = null
+      Files.walk(path).sorted(java.util.Collections.reverseOrder()).forEach(
+        new java.util.function.Consumer[Path] {
+          override def accept(p: Path): Unit = {
+            try {
+              Files.delete(p)
+            } catch {
+              case e: Exception =>
+                logError(s"Failure while deleting file or directory: $p", e)
+                failure = e
+            }
+          }
+        })
+      if (throwOnError && (failure ne null)) throw failure
+    } else if (throwOnError) {
+      throw new java.io.IOException(s"File or directory does not exist: $path")
+    } else if (logIfNotExists) {
+      logInfo(s"File or directory does not exist: $path")
+    }
+  }
+
+  override def logInfo(msg: => String): Unit = super.logInfo(msg)
+
+  override def logWarning(msg: => String): Unit = super.logWarning(msg)
+
+  override def logError(msg: => String): Unit = super.logError(msg)
 }
 
 class ExecutorLocalRDD[T: ClassTag](_sc: SparkContext, blockManagerIds: Seq[BlockManagerId],
@@ -1083,17 +1114,17 @@ private[spark] class CoGroupExecutorLocalPartition(
   override def hashCode(): Int = idx
 }
 
-object ToolsCallbackInit extends Logging {
+object ToolsCallbackInit {
   final val toolsCallback: ToolsCallback = {
     try {
       val c = org.apache.spark.util.Utils.classForName(
         "io.snappydata.ToolsCallbackImpl$")
       val tc = c.getField("MODULE$").get(null).asInstanceOf[ToolsCallback]
-      logInfo("toolsCallback initialized")
+      Utils.logInfo("toolsCallback initialized")
       tc
     } catch {
       case _: ClassNotFoundException =>
-        logWarning("ToolsCallback couldn't be INITIALIZED. " +
+        Utils.logWarning("ToolsCallback couldn't be INITIALIZED. " +
             "DriverURL won't get published to others.")
         null
     }
diff --git a/core/src/main/scala/org/apache/spark/sql/hive/HiveClientUtil.scala b/core/src/main/scala/org/apache/spark/sql/hive/HiveClientUtil.scala
index 06a5f530c..ec8fb9203 100644
--- a/core/src/main/scala/org/apache/spark/sql/hive/HiveClientUtil.scala
+++ b/core/src/main/scala/org/apache/spark/sql/hive/HiveClientUtil.scala
@@ -16,6 +16,8 @@
  */
 package org.apache.spark.sql.hive
 
+import java.nio.file.Paths
+
 import com.gemstone.gemfire.internal.shared.SystemProperties
 import com.pivotal.gemfirexd.Attribute.{PASSWORD_ATTR, USERNAME_ATTR}
 import com.pivotal.gemfirexd.internal.engine.Misc
@@ -39,8 +41,19 @@ import org.apache.spark.{Logging, SparkConf, SparkContext}
  */
 object HiveClientUtil extends Logging {
 
+  val HIVE_TMPDIR = "./hive"
+
+  val HIVE_DEFAULT_SETTINGS = Map(ConfVars.SCRATCHDIR -> hivePath("scratch"),
+    ConfVars.LOCALSCRATCHDIR -> hivePath("local_scratch"),
+    ConfVars.DOWNLOADED_RESOURCES_DIR -> hivePath("resources"),
+    ConfVars.HIVEHISTORYFILELOC -> hivePath("query_logs"),
+    ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LOG_LOCATION -> hivePath("operation_logs"))
+
   ExternalStoreUtils.registerBuiltinDrivers()
 
+  private def hivePath(name: String): String =
+    Paths.get(s"$HIVE_TMPDIR/$name").toAbsolutePath.toString
+
   /**
    * Create a SnappyHiveExternalCatalog appropriate for the cluster.
    * The catalog internally initializes a hive client that is used to retrieve metadata from
@@ -98,6 +111,9 @@ object HiveClientUtil extends Logging {
     sparkConf.set("spark.sql.hive.metastore.isolation", "false")
     sparkConf.set(HiveUtils.HIVE_METASTORE_SHARED_PREFIXES, Seq(
       "io.snappydata.jdbc", "com.pivotal.gemfirexd.jdbc"))
+    for ((hiveVar, dirName) <- HiveClientUtil.HIVE_DEFAULT_SETTINGS) {
+      sparkConf.set(hiveVar.varname, dirName)
+    }
 
     val skipFlags = GfxdDataDictionary.SKIP_CATALOG_OPS.get()
     val oldSkipCatalogCalls = skipFlags.skipHiveCatalogCalls
@@ -127,8 +143,6 @@ object HiveClientUtil extends Logging {
     // The DBCP 1.x versions are thoroughly outdated and should not be used but
     // the expectation is that the one bundled in datanucleus will be in better shape.
     metadataConf.setVar(ConfVars.METASTORE_CONNECTION_POOLING_TYPE, "dbcp-builtin")
-    // set the scratch dir inside current working directory (unused but created)
-    setDefaultPath(metadataConf, ConfVars.SCRATCHDIR, "./hive")
     metadataConf.setVar(ConfVars.HADOOPFS, "file:///")
     metadataConf.set("datanucleus.connectionPool.testSQL", "VALUES(1)")
 
@@ -155,16 +169,6 @@ object HiveClientUtil extends Logging {
     metadataConf.set("datanucleus.connectionPool.maxWait", "30000")
   }
 
-  private def setDefaultPath(metadataConf: SnappyHiveConf, v: ConfVars, path: String): String = {
-    var pathUsed = metadataConf.get(v.varname)
-    if ((pathUsed eq null) || pathUsed.isEmpty || pathUsed.equals(v.getDefaultExpr)) {
-      // set the path to provided
-      pathUsed = new java.io.File(path).getAbsolutePath
-      metadataConf.setVar(v, pathUsed)
-    }
-    pathUsed
-  }
-
   private def resolveMetaStoreDBProps(clusterMode: ClusterMode): (String, String) = {
     clusterMode match {
       case ThinClientConnectorMode(_, _) =>
diff --git a/core/src/main/scala/org/apache/spark/sql/hive/SnappyHiveExternalCatalog.scala b/core/src/main/scala/org/apache/spark/sql/hive/SnappyHiveExternalCatalog.scala
index a37d4f3c6..c70b8190c 100644
--- a/core/src/main/scala/org/apache/spark/sql/hive/SnappyHiveExternalCatalog.scala
+++ b/core/src/main/scala/org/apache/spark/sql/hive/SnappyHiveExternalCatalog.scala
@@ -37,7 +37,6 @@ import com.pivotal.gemfirexd.internal.engine.distributed.utils.GemFireXDUtils
 import com.pivotal.gemfirexd.internal.impl.sql.catalog.GfxdDataDictionary
 import io.snappydata.sql.catalog.SnappyExternalCatalog._
 import io.snappydata.sql.catalog.{CatalogObjectType, ConnectorExternalCatalog, RelationInfo, SnappyExternalCatalog}
-import org.apache.commons.io.FileUtils
 import org.apache.hadoop.conf.Configuration
 import org.apache.hadoop.hive.ql.metadata.Hive
 import org.apache.log4j.{Level, LogManager}
@@ -790,8 +789,6 @@ object SnappyHiveExternalCatalog {
       log4jLogger.setLevel(Level.ERROR)
     }
     try {
-      // delete the hive scratch directory if it exists
-      FileUtils.deleteDirectory(new java.io.File("./hive"))
       instance = new SnappyHiveExternalCatalog(sparkConf, hadoopConf, createTime)
     } finally {
       logger.setLevel(previousLevel)
